{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba4c02d",
   "metadata": {},
   "source": [
    "# Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch\n",
    "\n",
    "#### [Download notebook](https://github.com/opensearch-project/opensearch-py-ml/blob/main/docs/source/examples/demo_transformer_model_train_save_upload_to_openSearch.ipynb)\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook introduces the technique of synthetic data generation and how it can be used to obtain a deep learning model for Search that is custom built for a given set of documents. \n",
    "\n",
    "Deep learning models are very powerful and have been shown to improve state of the art in several disciplines and tasks. However, they need a lot of labelled training data. Such data is often hard to obtain.  In this notebook, we show how pre-trained large language models can be used to circumvent this issue. \n",
    "\n",
    "We focus on the task of passage retrieval i.e the corpus consists of passages which is searched at run-time given a user query. This search can be performed by transformers such as BERT as long as BERT is trained on a labelled dataset that consists of pairs such as (queries, relevant passage). Such a BERT model can be used for semantic search. \n",
    "\n",
    "\n",
    "### Synthetic query generation\n",
    "\n",
    "In the absence of such labelled data we provide a synthetic query generator (SQG) model that can be used to create synthetic queries given a passage. The SQG model is a large transformer model that has been trained to generate human like queries given a passage. Thus it can be used to create a labelled dataset of (synthetic queries, passage). A BERT model can be trained on this synthetic data and used for semantic search. In fact, we find that such synthetically trained models beat the current state-of-the-art models. Note that resulting BERT model is a customized model since it has been trained on a specific corpus (and corresponding synthetic queries).\n",
    "\n",
    "\n",
    "This notebook provides an end-to-end guide for users to generate synthetic queries and fine-tune a sentence transformer model on it using opensearch_py_ml. It consists of the following steps,\n",
    "\n",
    "Step 1: Import packages and set up client\n",
    "\n",
    "Step 2: Import the data/passages for synthetic query generation\n",
    "\n",
    "Step 3: Generate Synthetic Queries\n",
    "\n",
    "Step 4: Read synthetic queries and train/fine-tune model using a hugging face sentence transformer model\n",
    "\n",
    "Step 5: Upload the model to OpenSearch cluster\n",
    "\n",
    "Steps 3 and 4 are compute intensive step, and we recommend running it on a machine with 4 or more GPUS such as the EC2 `p3.8xlarge` or `p3.16xlarge`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011727e",
   "metadata": {},
   "source": [
    "## Step 1: Import packages, set up client and define helper functions\n",
    "\n",
    "Install required packages for opensearch_py_ml.sentence_transformer_model\n",
    "Install `opensearchpy` and `opensearch-py-ml` through pypi\n",
    "\n",
    "generate.py script is released with the Synthetic Query Generation model.\n",
    "\n",
    "Please refer https://pytorch.org/ to proper install torch based on your environment setting.\n",
    "\n",
    "Please install the following packages from the terminal if you haven't already. They can be also installed from the notebook by uncommenting the line and execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a4546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas<3,>=1.5.2\n",
      "  Using cached pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Collecting matplotlib<4,>=3.6.2\n",
      "  Using cached matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Collecting numpy<2,>=1.24.0\n",
      "  Using cached numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting opensearch-py>=2.2.0\n",
      "  Using cached opensearch_py-2.3.1-py2.py3-none-any.whl (327 kB)\n",
      "Collecting torch==2.0.1\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting onnx\n",
      "  Using cached onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Collecting deprecated\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting mdutils\n",
      "  Using cached mdutils-1.6.0-py3-none-any.whl\n",
      "Collecting pytest>=7.1.2\n",
      "  Using cached pytest-7.4.2-py3-none-any.whl (324 kB)\n",
      "Collecting pytest-mock\n",
      "  Using cached pytest_mock-3.11.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting pytest-cov\n",
      "  Using cached pytest_cov-4.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting nbval\n",
      "  Using cached nbval-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pywavelets\n",
      "  Using cached PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Collecting nox\n",
      "  Using cached nox-2023.4.22-py3-none-any.whl (54 kB)\n",
      "Collecting mypy==1.3.0\n",
      "  Using cached mypy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "Collecting sphinx==6.1.3\n",
      "  Using cached sphinx-6.1.3-py3-none-any.whl (3.0 MB)\n",
      "Collecting sphinx-rtd-theme==1.2.2\n",
      "  Using cached sphinx_rtd_theme-1.2.2-py2.py3-none-any.whl (2.8 MB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting triton==2.0.0\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting tomli>=1.1.0\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting mypy-extensions>=1.0.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Using cached sphinxcontrib_applehelp-1.0.7-py3-none-any.whl (120 kB)\n",
      "Collecting docutils<0.20,>=0.18\n",
      "  Using cached docutils-0.19-py3-none-any.whl (570 kB)\n",
      "Collecting imagesize>=1.3\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: packaging>=21.0 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from sphinx==6.1.3->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 33)) (23.2)\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Using cached sphinxcontrib_qthelp-1.0.6-py3-none-any.whl (89 kB)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Using cached alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5\n",
      "  Using cached sphinxcontrib_serializinghtml-1.1.9-py3-none-any.whl (92 kB)\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Using cached sphinxcontrib_devhelp-1.0.5-py3-none-any.whl (83 kB)\n",
      "Collecting snowballstemmer>=2.0\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Collecting babel>=2.9\n",
      "  Using cached Babel-2.13.0-py3-none-any.whl (10.1 MB)\n",
      "Collecting requests>=2.25.0\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: Pygments>=2.13 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from sphinx==6.1.3->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 33)) (2.16.1)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Using cached sphinxcontrib_htmlhelp-2.0.4-py3-none-any.whl (99 kB)\n",
      "Collecting docutils<0.20,>=0.18\n",
      "  Using cached docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.41.2-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: setuptools in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 8)) (65.5.0)\n",
      "Collecting cmake\n",
      "  Using cached cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "Collecting lit\n",
      "  Using cached lit-17.0.2-py3-none-any.whl\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from pandas<3,>=1.5.2->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 4)) (2.8.2)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.43.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-10.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Collecting certifi>=2022.12.07\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Requirement already satisfied: six in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from opensearch-py>=2.2.0->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 7)) (1.16.0)\n",
      "Collecting urllib3<2,>=1.21.1\n",
      "  Using cached urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
      "Collecting protobuf>=3.20.2\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Requirement already satisfied: psutil in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from accelerate->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 10)) (5.9.5)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting tokenizers<0.15,>=0.14\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from pytest>=7.1.2->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 20)) (1.1.3)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Using cached pluggy-1.3.0-py3-none-any.whl (18 kB)\n",
      "Collecting iniconfig\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting coverage[toml]>=5.2.1\n",
      "  Using cached coverage-7.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Requirement already satisfied: ipykernel in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (6.25.2)\n",
      "Collecting nbformat\n",
      "  Using cached nbformat-5.9.2-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: jupyter-client in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (8.3.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting colorlog<7.0.0,>=2.6.1\n",
      "  Using cached colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting argcomplete<4.0,>=1.9.4\n",
      "  Using cached argcomplete-3.1.2-py3-none-any.whl (41 kB)\n",
      "Collecting virtualenv>=14\n",
      "  Using cached virtualenv-20.24.5-py3-none-any.whl (3.7 MB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting distlib<1,>=0.3.7\n",
      "  Using cached distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
      "Requirement already satisfied: platformdirs<4,>=3.9.1 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from virtualenv>=14->nox->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 31)) (3.11.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.1.6)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (5.3.2)\n",
      "Requirement already satisfied: pyzmq>=20 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (25.1.1)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (8.16.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (5.11.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.1.4)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (6.3.3)\n",
      "Requirement already satisfied: nest-asyncio in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (1.5.8)\n",
      "Collecting jsonschema>=2.6\n",
      "  Using cached jsonschema-4.19.1-py3-none-any.whl (83 kB)\n",
      "Collecting fastjsonschema\n",
      "  Using cached fastjsonschema-2.18.1-py3-none-any.whl (23 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (3.0.39)\n",
      "Requirement already satisfied: backcall in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.7.5)\n",
      "Requirement already satisfied: stack-data in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.6.3)\n",
      "Requirement already satisfied: decorator in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (5.1.1)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.30.2-py3-none-any.whl (25 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
      "Collecting attrs>=22.2.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.2.8)\n",
      "Requirement already satisfied: pure-eval in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (0.2.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (2.0.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/kalyan/oss/opensearch/opensearch-py-ml/venv1/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->nbval->-r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt (line 23)) (2.4.0)\n",
      "Installing collected packages: snowballstemmer, sentencepiece, pytz, mpmath, mdutils, lit, fastjsonschema, distlib, cmake, wrapt, wheel, urllib3, tzdata, typing-extensions, tqdm, tomli, threadpoolctl, sympy, sphinxcontrib-jsmath, safetensors, rpds-py, regex, pyyaml, pyparsing, protobuf, pluggy, pillow, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, networkx, mypy-extensions, MarkupSafe, kiwisolver, joblib, iniconfig, imagesize, idna, fsspec, fonttools, filelock, docutils, cycler, coverage, colorlog, click, charset-normalizer, certifi, babel, attrs, argcomplete, alabaster, virtualenv, scipy, requests, referencing, pywavelets, pytest, pandas, onnx, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nltk, mypy, jinja2, deprecated, contourpy, scikit-learn, pytest-mock, pytest-cov, opensearch-py, nvidia-cusolver-cu11, nvidia-cudnn-cu11, nox, matplotlib, jsonschema-specifications, huggingface-hub, tokenizers, jsonschema, transformers, nbformat, nbval, triton, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, torch, sphinx, torchvision, sphinxcontrib-jquery, sphinx-rtd-theme, sentence_transformers, accelerate\n",
      "\u001b[33m  DEPRECATION: sentence_transformers is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for sentence_transformers ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed MarkupSafe-2.1.3 accelerate-0.23.0 alabaster-0.7.13 argcomplete-3.1.2 attrs-23.1.0 babel-2.13.0 certifi-2023.7.22 charset-normalizer-3.3.0 click-8.1.7 cmake-3.27.6 colorlog-6.7.0 contourpy-1.1.1 coverage-7.3.2 cycler-0.12.1 deprecated-1.2.14 distlib-0.3.7 docutils-0.18.1 fastjsonschema-2.18.1 filelock-3.12.4 fonttools-4.43.1 fsspec-2023.9.2 huggingface-hub-0.17.3 idna-3.4 imagesize-1.4.1 iniconfig-2.0.0 jinja2-3.1.2 joblib-1.3.2 jsonschema-4.19.1 jsonschema-specifications-2023.7.1 kiwisolver-1.4.5 lit-17.0.2 matplotlib-3.8.0 mdutils-1.6.0 mpmath-1.3.0 mypy-1.3.0 mypy-extensions-1.0.0 nbformat-5.9.2 nbval-0.10.0 networkx-3.1 nltk-3.8.1 nox-2023.4.22 numpy-1.26.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 onnx-1.14.1 opensearch-py-2.3.1 pandas-2.1.1 pillow-10.0.1 pluggy-1.3.0 protobuf-4.24.4 pyparsing-3.1.1 pytest-7.4.2 pytest-cov-4.1.0 pytest-mock-3.11.1 pytz-2023.3.post1 pywavelets-1.4.1 pyyaml-6.0.1 referencing-0.30.2 regex-2023.10.3 requests-2.31.0 rpds-py-0.10.4 safetensors-0.4.0 scikit-learn-1.3.1 scipy-1.11.3 sentence_transformers-2.2.2 sentencepiece-0.1.99 snowballstemmer-2.2.0 sphinx-6.1.3 sphinx-rtd-theme-1.2.2 sphinxcontrib-applehelp-1.0.7 sphinxcontrib-devhelp-1.0.5 sphinxcontrib-htmlhelp-2.0.4 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.6 sphinxcontrib-serializinghtml-1.1.9 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.14.1 tomli-2.0.1 torch-2.0.1 torchvision-0.15.2 tqdm-4.66.1 transformers-4.34.0 triton-2.0.0 typing-extensions-4.8.0 tzdata-2023.3 urllib3-1.26.17 virtualenv-20.24.5 wheel-0.41.2 wrapt-1.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r https://raw.githubusercontent.com/opensearch-project/opensearch-py-ml/main/requirements-dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e239ffc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('generate.py', <http.client.HTTPMessage at 0x10bdbe940>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download generate.py for Generate Synthetic Queries\n",
    "\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://artifacts.opensearch.org/models/ml-models/amazon/gpt/GPT2_xl_sqg/1.0.0/generate.py\", \"generate.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17a3e085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install pandas matplotlib numpy torch accelerate sentence_transformers tqdm transformers opensearch-py opensearch-py-ml detoxify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c021df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "import opensearch_py_ml as oml\n",
    "from opensearchpy import OpenSearch\n",
    "import generate \n",
    "from generate import Synthetic_Query_Generation\n",
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "import boto3, json\n",
    "import pandas as pd, numpy as np\n",
    "from datasets import load_dataset\n",
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "798cac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlcommon to later upload the model to OpenSearch Cluster\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c85ae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_URL = 'https://localhost:9200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77442abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_os_client(cluster_url = CLUSTER_URL,\n",
    "                  username='admin',\n",
    "                  password='admin'):\n",
    "    '''\n",
    "    Get OpenSearch client\n",
    "    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443\n",
    "    :return: OpenSearch client\n",
    "    '''\n",
    "    client = OpenSearch(\n",
    "        hosts=[cluster_url],\n",
    "        http_auth=(username, password),\n",
    "        verify_certs=False\n",
    "    )\n",
    "    return client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89e1cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_os_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc33a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myselect(x):    \n",
    "    if max(x[\"passages\"][\"is_selected\"]) == 1:\n",
    "        return x[\"passages\"][\"passage_text\"][np.argmax(x[\"passages\"][\"is_selected\"])]\n",
    "    return \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2294d0",
   "metadata": {},
   "source": [
    "## Step 2: Import the data/passages for synthetic query generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414dce7",
   "metadata": {},
   "source": [
    "There are three supported options to read datasets :\n",
    "\n",
    "* Option 1: read from a local data folder in jsonl file \n",
    "\n",
    "* Option 2: read from a list of passages\n",
    "\n",
    "* Option 3: read from OpenSearch client by index_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58882e",
   "metadata": {},
   "source": [
    "For the purpose of this notebook we will demonstrate option 2: read from a list of passages. \n",
    "\n",
    "We take the MS Marco dataset of passages as our example dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d19cd5",
   "metadata": {},
   "source": [
    "### 2.1) Load the data and convert into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe008eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\",\"v1.1\")\n",
    "df = pd.DataFrame.from_dict(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1927c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"passage\"] = df.apply(lambda x: myselect(x), axis = 1)\n",
    "df = df[[\"query\",\"passage\"]][df.passage != \"-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659063f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting print options to display full columns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', None)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68580786",
   "metadata": {},
   "source": [
    "The dataset looks like,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b03ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee32b5",
   "metadata": {},
   "source": [
    "The MS Marco dataset has real queries for passages but we will pretend that it does not and generate synthetic queries for each passage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e61085",
   "metadata": {},
   "source": [
    "### 2.2) Convert the data into a list of strings and instantiate an object of the class Synthetic_Query_Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_passages = list(df.passage.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = Synthetic_Query_Generation(sentences = sample_passages[:8]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b9e05",
   "metadata": {},
   "source": [
    "## Step 3: Generate synthetic queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_step_query = ss.generate_synthetic_queries(num_machines = 1,\n",
    "                                                 tokenize_data = True,\n",
    "                                                 tokenizer_max_length =  300, \n",
    "                                                 total_queries = 10,\n",
    "                                                 numseq = 5,\n",
    "                                                 num_gpu = 0,\n",
    "                                                 toxic_cutoff = 0.01, \n",
    "                                                 tokens_to_word_ratio = 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519d080",
   "metadata": {},
   "source": [
    "A lot of actions are being executed in the above cell. We elaborate them step by step, \n",
    "\n",
    "    1) Convert the data into a form that can be consumed by the Synthetic query generator (SQG) model. This amounts to tokenizing the data using a tokenizer. The SQG model is a fine-tuned version of the GPT-XL model https://huggingface.co/gpt2-xl and the tokenizer is the GPT tokenizer.\n",
    "\n",
    "    2) The tokenizer has a max input length of 512 tokens. Every passage is tokenized with the special tokens <|startoftext|> and QRY: appended to the beginning and the end of every passage respectively. Note that tokenization is a time intensive process and the script saves the tokenized data after the first pass. We recommend setting tokenize_data = False subsequently.\n",
    "\n",
    "    3) Load the SQG model i.e. 1.5B parameter GPT2-XL model that has been trained to ask questions given passages. This model has been made publicly available and can be found here: https://artifacts.opensearch.org/models/ml-models/amazon/gpt/GPT2_xl_sqg/1.0.0/GPT2_xl_sqg.zip\n",
    "\n",
    "    4) Once the model has been loaded and the data has been tokenized, the model starts the process of query generation. \"total_queries\" is number of synthetic queries generated for every passage and \"numseq\" is the number of queries that are generated by a model at a given time. Ideally total_queries = numseq, but this can lead to out of memory issues. So set numseq to an integer that is around 10 or less, and is a divisor of total_queries.\n",
    "\n",
    "    5) tokens_to_word_ratio is a float variable that is used to switch between length of a document in tokens vs. in words. It is used when truncating documents during the tokenization phase. Most words are split in to one or more tokens. A document that has a length of 300 tokens might only be 200 words long. This ratio of 200/300 = 2/3 = 0.667 is the tokens_to_word_ratio. For passages from a dataset such as Wikipedia this ratio is around 0.65 to 0.7, but for domain specific datasets this ratio could be as small as 0.5.\n",
    "    \n",
    "    6) The script also requires to know the number of GPUs and the number of machines/nodes that it can use. Since we are using a single node instance with no GPUs we pass 0 and 1 to the function respectively. Our recommended setting is to use 1 machine/node with at least 4 (ideally 8) GPUs.\n",
    "\n",
    "    7) The script now begins to generate queries and displays a progress bar. We create total_queries per passage. Empirically we find that generating more queries leads to better performance but there are diminishing returns since the total inference time increases with total_queries.\n",
    "\n",
    "    8) After generating the queries, the function uses a publicly available package called Detoxify to remove inappropriate queries from the dataset. \"toxic_cutoff\" is a float. The script rejects all queries that have a toxicity score greater than toxic_cutoff\n",
    "\n",
    "    9) Finally, the synthetic queries along with their corresponding passages are saved in a zipped file in the current working directory.\n",
    "\n",
    "Note -- Please restart the kernel and rerun it if the notebook gives CUDA related errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c09364",
   "metadata": {},
   "source": [
    "### This is how the sample queries look like, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a7c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate SentenceTransformerModel object\n",
    "\n",
    "custom_model = SentenceTransformerModel(folder_path=\"/Volumes/workplace/upload_content/model_files/\", overwrite = True)\n",
    "\n",
    "\n",
    "\n",
    "df = custom_model.read_queries(read_path = '/Volumes/workplace/upload_content/clean_synthetic_queries.zip', overwrite = True)\n",
    "\n",
    "df[::10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9e0de",
   "metadata": {},
   "source": [
    "## Step 4: Read synthetic queries and train/fine-tune a hugging face sentence transformer model on synthetic data\n",
    "\n",
    "With a synthetic queries zip file, users can fine tune a sentence transformer model. \n",
    "\n",
    "The `SentenceTransformerModel` class will inititate an object for training, exporting and configuring the model. Plese visit the [SentenceTransformerModel](https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.html#opensearch_py_ml.sentence_transformer_model.SentenceTransformerModel) for API Reference . \n",
    "\n",
    "The `train` function will import synthestic queries, load sentence transformer example and train the model using a hugging face sentence transformer model. Plese visit the [SentenceTransformerModel.train](https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.html#opensearch_py_ml.sentence_transformer_model.SentenceTransformerModel.train) for API Reference . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a337ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up cache before training to free up spaces\n",
    "import gc, torch\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = custom_model.train(read_path = '/Volumes/workplace/upload_content/clean_synthetic_queries.zip',\n",
    "                        output_model_name = 'test2_model.pt',\n",
    "                        zip_file_name= 'test2_model.zip',\n",
    "                        overwrite = True,\n",
    "                        num_epochs = 10,\n",
    "                        verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af60a71",
   "metadata": {},
   "source": [
    "Following are some important points about the training cell executed above,\n",
    "\n",
    "1. The input to the training script consists of (query, passage) pairs. The model is trained to maximize the dot product between relevant queries and passages while at the same time minimize the dot product between queries and irrelevant passages. This is also known as contrastive learning. We implement this using in-batch negatives and a symmetric loss as mentioned below. \n",
    "\n",
    "2. To utilize the power of GPUs we collect training samples into a batch before sending for model training. Each batch contains B number of randomly selected training samples (q, p). Thus within a batch each query has one relevant passage and B-1 irrelevant passages. Similarly for every passage there's one relevant query and B-1 irrelevant queries. For every given relevant query and passage pair we minimize the following expression, called the loss, \n",
    "\n",
    "3. For a given batch B, the loss is defined as loss = C(q, p) + C(p, q) where $C(q, p) = - \\sum_{i=1}^{i=B} \\log \\left( \\frac{exp(q_i \\cdot p_i)}{\\sum_{j=1} ^{B} exp(q_i \\cdot p_j)}\\right)$   \n",
    "\n",
    "4. The model truncates documents beyond 512 tokens. If the corpus contains documents that are shorter than 512 tokens the model max length can be adjusted to that number. Shorter sequences take less memory and therefore allow for bigger batch sizes. The max length can be adjusted by the \"percentile\" argument.  \n",
    "\n",
    "5. We use a batch size of 32 per device. Larger batch sizes lead to more in-batch negative samples and lead to better performance but unfortunately they also lead to out of memory issues. Shorter sequences use less memory, so if the document corpus is short feel free to experiment with larger batch sizes.\n",
    "\n",
    "6. The model is trained using the AdamW optimizer for 10 epochs with a learning rate of 2e-5 and a scheduler with linear schedule with warmup steps = 10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd0405",
   "metadata": {},
   "source": [
    "## Step 5: Upload the model to OpenSearch cluster\n",
    "After generated a model zip file, the users will need to describe model configuration in a ml-commons_model_config.json file. The `make_model_config_json` function in sentencetransformermodel class will parse the config file from hugging-face config.son file. If users would like to use a different config than the pre-trained sentence transformer, `make_model_config_json` function provide arguuments to change the configuration content and generated a ml-commons_model_config.json file. Plese visit the [SentenceTransformerModel.make_model_config_json](https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.html#opensearch_py_ml.sentence_transformer_model.SentenceTransformerModel.make_model_config_json) for API Reference . \n",
    "\n",
    "In general, the ml common client supports uploading sentence transformer models. With a zip file contains model in  Torch Script format, and a configuration file for tokenizers in json format, the `upload_model` function connects to opensearch through ml client and upload the model. Plese visit the [MLCommonClient.upload_model](https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons_integration.MLCommonClient.upload_model) for API Reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe84425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#users will need to prepare a ml-commons_model_config.json file to config the model, including model name ..\n",
    "#this is a helpful function in py-ml.sentence_transformer_model to generate ml-commons_model_config.json file\n",
    "custom_model.make_model_config_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9cc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to ml_common client with OpenSearch client\n",
    "import opensearch_py_ml as oml\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "ml_client = MLCommonClient(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7b0ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks 10\n",
      "Sha1 value of the model file:  61fd5a1425960681da49d084dca0e52fd0fabcc0f2e1c4d57c4e20e193bde483\n",
      "Model meta data was created successfully. Model Id:  lGFG9IUBTo3f8n5R8nM6\n",
      "uploading chunk 1 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 2 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 3 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 4 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 5 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 6 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 7 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 8 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 9 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "uploading chunk 10 of 10\n",
      "Model id: {'status': 'Uploaded'}\n",
      "Model uploaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lGFG9IUBTo3f8n5R8nM6'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload model to OpenSearch cluster, using model zip file path and ml-commons_model_config.json file generated above\n",
    "\n",
    "model_path = '/Volumes/workplace/upload_content/all-MiniLM-L6-v2.zip'\n",
    "model_config_path = '/Volumes/workplace/upload_content/model_config.json'\n",
    "ml_client.upload_model( model_path, model_config_path, isVerbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
